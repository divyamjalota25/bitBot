{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cddd4cda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: ollama 0.3.3\n",
      "Uninstalling ollama-0.3.3:\n",
      "  Successfully uninstalled ollama-0.3.3\n",
      "Found existing installation: langchain 0.3.7\n",
      "Uninstalling langchain-0.3.7:\n",
      "  Successfully uninstalled langchain-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall ollama -y\n",
    "!pip uninstall langchain -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c3a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting ollama\n",
      "  Downloading ollama-0.3.3-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: httpx<0.28.0,>=0.27.0 in /home/student/venv/lib/python3.11/site-packages (from ollama) (0.27.0)\n",
      "Requirement already satisfied: httpcore==1.* in /home/student/venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/student/venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.4)\n",
      "Requirement already satisfied: anyio in /home/student/venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (3.7.1)\n",
      "Requirement already satisfied: sniffio in /home/student/venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (1.3.0)\n",
      "Requirement already satisfied: certifi in /home/student/venv/lib/python3.11/site-packages (from httpx<0.28.0,>=0.27.0->ollama) (2023.7.22)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/student/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama) (0.14.0)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.3.3\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting langchain\n",
      "  Downloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /home/student/venv/lib/python3.11/site-packages (from langchain) (3.9.5)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/student/venv/lib/python3.11/site-packages (from langchain) (6.0.1)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /home/student/venv/lib/python3.11/site-packages (from langchain) (0.3.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /home/student/venv/lib/python3.11/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/student/venv/lib/python3.11/site-packages (from langchain) (1.4.53)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /home/student/venv/lib/python3.11/site-packages (from langchain) (2.9.2)\n",
      "Requirement already satisfied: requests<3,>=2 in /home/student/venv/lib/python3.11/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.15 in /home/student/venv/lib/python3.11/site-packages (from langchain) (0.3.15)\n",
      "Requirement already satisfied: numpy<2,>=1 in /home/student/venv/lib/python3.11/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/student/venv/lib/python3.11/site-packages (from langchain) (0.1.142)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/student/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/student/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/student/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/student/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/student/venv/lib/python3.11/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/student/venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /home/student/venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (4.12.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/student/venv/lib/python3.11/site-packages (from langchain-core<0.4.0,>=0.3.15->langchain) (1.33)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/student/venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/student/venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/student/venv/lib/python3.11/site-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/student/venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /home/student/venv/lib/python3.11/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/student/venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/student/venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/student/venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (3.2.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/student/venv/lib/python3.11/site-packages (from requests<3,>=2->langchain) (2.0.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/student/venv/lib/python3.11/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
      "Requirement already satisfied: anyio in /home/student/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /home/student/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /home/student/venv/lib/python3.11/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/student/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/student/venv/lib/python3.11/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.15->langchain) (2.4)\n",
      "Installing collected packages: langchain\n",
      "Successfully installed langchain-0.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade ollama\n",
    "!pip install --upgrade langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7783c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -U langchain-ollama langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb0ff047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from langchain.document_loaders.pdf import PyPDFDirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_chroma import Chroma  # Updated import for Chroma\n",
    "from langchain_ollama import OllamaEmbeddings  # Updated import for OllamaEmbeddings\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Updated paths\n",
    "CHROMA_PATH = \"/home/student/proj/learn/Chatbot_RAG/chroma\"\n",
    "DATA_PATH = \"/home/student/proj/learn/Chatbot_RAG/RAG_Documents\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "094de724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_embedding_function():\n",
    "#     return OllamaEmbeddings(model=\"vicuna:13b-q8_0\",base_url=\"http://localhost:11434\")  # Updated model name as per requirements\n",
    "\n",
    "# def clear_database():\n",
    "#     if os.path.exists(CHROMA_PATH):\n",
    "#         print(\"Clearing existing Chroma database...\")\n",
    "#         shutil.rmtree(CHROMA_PATH)\n",
    "#         print(\"Database cleared.\")\n",
    "\n",
    "# def load_documents():\n",
    "#     document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "#     print(\"Loading documents from PDF files...\")\n",
    "#     documents = document_loader.load()\n",
    "#     print(f\"Loaded {len(documents)} documents.\")\n",
    "#     return documents\n",
    "\n",
    "# def split_documents(documents):\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(\n",
    "#         chunk_size=1000,\n",
    "#         chunk_overlap=50,\n",
    "#         length_function=len,\n",
    "#         is_separator_regex=False,\n",
    "#     )\n",
    "#     print(\"Splitting documents into chunks...\")\n",
    "#     chunks = text_splitter.split_documents(documents)\n",
    "#     print(f\"Generated {len(chunks)} chunks.\")\n",
    "#     return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d35669fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_documents_in_parallel(documents):\n",
    "    print(\"Processing documents in parallel...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(process_single_document, documents), total=len(documents)))\n",
    "    chunks = [chunk for result in results for chunk in result]\n",
    "    print(f\"Total chunks after parallel processing: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def process_single_document(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33ca8152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_function():\n",
    "    return OllamaEmbeddings(model=\"llama3:latest\")  # Updated model name as per requirements\n",
    "\n",
    "def clear_database():\n",
    "    if os.path.exists(CHROMA_PATH):\n",
    "        print(\"Clearing existing Chroma database...\")\n",
    "        shutil.rmtree(CHROMA_PATH)\n",
    "        print(\"Database cleared.\")\n",
    "\n",
    "def load_documents():\n",
    "    document_loader = PyPDFDirectoryLoader(DATA_PATH)\n",
    "    print(\"Loading documents from PDF files...\")\n",
    "    documents = document_loader.load()\n",
    "    print(f\"Loaded {len(documents)} documents.\")\n",
    "    return documents\n",
    "\n",
    "def split_documents(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Generated {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "def process_documents_in_parallel(documents):\n",
    "    print(\"Processing documents in parallel...\")\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(process_single_document, documents), total=len(documents)))\n",
    "    chunks = [chunk for result in results for chunk in result]\n",
    "    print(f\"Total chunks after parallel processing: {len(chunks)}\")\n",
    "    return chunks\n",
    "\n",
    "def process_single_document(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=50,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    return text_splitter.split_documents([document])\n",
    "\n",
    "def add_to_chroma(chunks):\n",
    "    db = Chroma(\n",
    "        persist_directory=CHROMA_PATH,\n",
    "        embedding_function=get_embedding_function()\n",
    "    )\n",
    "    \n",
    "    print(\"Calculating unique chunk IDs...\")\n",
    "    chunks_with_ids = calculate_chunk_ids(chunks)\n",
    "\n",
    "    # Retrieve existing document IDs in the database\n",
    "    existing_items = db.get(include=[])\n",
    "    existing_ids = set(existing_items[\"ids\"])\n",
    "    print(f\"Number of existing documents in DB: {len(existing_ids)}\")\n",
    "\n",
    "    # Filter out chunks that already exist in the database\n",
    "    new_chunks = [chunk for chunk in chunks_with_ids if chunk.metadata[\"id\"] not in existing_ids]\n",
    "    print(f\"Number of new documents to add: {len(new_chunks)}\")\n",
    "\n",
    "    # Batch processing to optimize memory usage\n",
    "    batch_size = 99  # Adjust batch size based on system resources\n",
    "    for i in range(0, len(new_chunks), batch_size):\n",
    "        batch = new_chunks[i:i + batch_size]\n",
    "        new_chunk_ids = [chunk.metadata[\"id\"] for chunk in batch]\n",
    "        print(f\"Adding batch {i // batch_size + 1} with {len(batch)} chunks to the database...\")\n",
    "        db.add_documents(batch, ids=new_chunk_ids)\n",
    "\n",
    "        # Removed db.persist() call as it may not be needed\n",
    "\n",
    "def calculate_chunk_ids(chunks):\n",
    "    last_page_id = None\n",
    "    current_chunk_index = 0\n",
    "    for chunk in chunks:\n",
    "        source = chunk.metadata.get(\"source\")\n",
    "        page = chunk.metadata.get(\"page\")\n",
    "        current_page_id = f\"{source}:{page}\"\n",
    "        if current_page_id == last_page_id:\n",
    "            current_chunk_index += 1\n",
    "        else:\n",
    "            current_chunk_index = 0\n",
    "        chunk_id = f\"{current_page_id}:{current_chunk_index}\"\n",
    "        last_page_id = current_page_id\n",
    "        chunk.metadata[\"id\"] = chunk_id\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45c2dfb6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from PDF files...\n",
      "Loaded 1946 documents.\n",
      "Processing documents in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 60963.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks after parallel processing: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating unique chunk IDs...\n",
      "Number of existing documents in DB: 0\n",
      "Number of new documents to add: 20\n",
      "Adding batch 1 with 20 chunks to the database...\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    clear_database()\n",
    "    documents = load_documents()[:5]  # Limit to 5 documents for faster testing; adjust as needed\n",
    "    chunks = process_documents_in_parallel(documents)\n",
    "    add_to_chroma(chunks)\n",
    "\n",
    "# Run the main function\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d80504b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores.chroma import Chroma\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_community.llms.ollama import Ollama\n",
    "#from get_embedding_function import get_embedding_function_vicuna as get_embedding_function\n",
    "\n",
    "\n",
    "CHROMA_PATH = \"/home/student/proj/learn/Chatbot_RAG/chroma\"\n",
    "\n",
    "#Strictly answer the question in 4-5 sentence in moderate detail.\n",
    "#Strictly answer the questions in 2-3 sentences unless asked by the user to answer in detail. In case the use asks for a detailed answer, always keep the answer length in 4-5 sentences. \n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Strictly answer the question in 4 sentences.\n",
    "\n",
    "----\n",
    "\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\"\n",
    "\n",
    "def query_rag(query_text: str):\n",
    "    # Prepare the DB.\n",
    "    embedding_function = get_embedding_function()\n",
    "    db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n",
    "\n",
    "    # Search the DB.\n",
    "    results = db.similarity_search_with_score(query_text, k=5)\n",
    "    \n",
    "    # Filter out any results with None for page_content\n",
    "    valid_results = [doc for doc, _score in results if doc.page_content is not None]\n",
    "\n",
    "    # Construct context text only with valid entries\n",
    "    context_text = \"\\n\\n---\\n\\n\".join([doc.page_content for doc in valid_results])\n",
    "    \n",
    "    prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "    \n",
    "    #change model name here, get exact model name using this command on terminal: ollama list\n",
    "    model = Ollama(model=\"vicuna:13b-q8_0\")\n",
    "    response_text = model.invoke(prompt)\n",
    "\n",
    "    sources = [doc.metadata.get(\"id\", None) for doc, _score in results]\n",
    "    formatted_response = f\"Response: {response_text}\\nSources: {sources}\"\n",
    "    print(\"Response from openhermes\", formatted_response)\n",
    "    return response_text\n",
    "\n",
    "# Directly set query_text for Jupyter environment\n",
    "#query_text = \"what is portfolio diversification in detail?\"\n",
    "#query_rag(query_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b9463163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0822a404",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import nltk\n",
    "# from rouge_score import rouge_scorer\n",
    "# from bert_score import score as bert_score\n",
    "# from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# # Ensure NLTK models are downloaded\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# # Function to calculate BLEU score\n",
    "# def calculate_bleu(reference, candidate):\n",
    "#     reference_tokens = [nltk.word_tokenize(reference)]\n",
    "#     candidate_tokens = nltk.word_tokenize(candidate)\n",
    "#     return sentence_bleu(reference_tokens, candidate_tokens)\n",
    "\n",
    "# # Function to calculate ROUGE-L score\n",
    "# def calculate_rouge(reference, candidate):\n",
    "#     scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "#     scores = scorer.score(reference, candidate)\n",
    "#     return scores['rougeL'].fmeasure\n",
    "\n",
    "# # Function to calculate BERTScore\n",
    "# def calculate_bertscore(references, candidates):\n",
    "#     P, R, F1 = bert_score(candidates, references, lang='en', rescale_with_baseline=True)\n",
    "#     return F1.mean().item()\n",
    "\n",
    "# weight_bleu = 0.3\n",
    "# weight_rouge = 0.3\n",
    "# weight_bertscore = 0.4\n",
    "\n",
    "# # Load/change CSV file\n",
    "# excel_path = 'Q_A.csv'  # Path to your CSV file\n",
    "# df = pd.read_csv(excel_path)\n",
    "\n",
    "# # Define column names based on your CSV file structure\n",
    "# question_column = 'Questions'         # Column containing questions\n",
    "# gold_answer_column = 'Answers'   # Column containing gold standard answers\n",
    "\n",
    "# # Lists to store evaluation metrics and model responses\n",
    "# bleu_scores = []\n",
    "# rouge_scores = []\n",
    "# bert_scores = []\n",
    "# final_scores = []\n",
    "# model_responses = []\n",
    "\n",
    "# # Loop through each question in the DataFrame\n",
    "# # Loop through each question in the DataFrame\n",
    "# for i, row in df.iterrows():\n",
    "#     question = row[question_column]\n",
    "#     gold_answer = row[gold_answer_column]\n",
    "\n",
    "#     # Generate the model's response for the question using query_rag function\n",
    "#     model_response = query_rag(question) or \"No response\"  # Use \"No response\" if the function returns None\n",
    "    \n",
    "#     # Strip leading/trailing whitespace or newlines\n",
    "#     model_response = model_response.strip()  \n",
    "#     print(f\"Question: {question}\\nModel Response: {model_response}\\n\")  # Print to check the response\n",
    "    \n",
    "#     model_responses.append(model_response)\n",
    "\n",
    "#     # Calculate BLEU, ROUGE, and BERTScore\n",
    "#     bleu = calculate_bleu(gold_answer, model_response)\n",
    "#     rouge = calculate_rouge(gold_answer, model_response)\n",
    "#     bertscore = calculate_bertscore([gold_answer], [model_response])\n",
    "#     final_score = (\n",
    "#             weight_bleu * bleu +\n",
    "#             weight_rouge * rouge +\n",
    "#             weight_bertscore * bertscore\n",
    "#         )\n",
    "    \n",
    "#     # Append scores to respective lists\n",
    "#     bleu_scores.append(bleu)\n",
    "#     rouge_scores.append(rouge)\n",
    "#     bert_scores.append(bertscore)\n",
    "#     final_scores.append(final_score)\n",
    "\n",
    "# # Write model responses and scores back to DataFrame\n",
    "# df['Model Response'] = model_responses\n",
    "# df['BLEU Score'] = bleu_scores\n",
    "# df['ROUGE-L Score'] = rouge_scores\n",
    "# df['BERTScore'] = bert_scores\n",
    "# df['Final Score'] = final_scores\n",
    "\n",
    "# # Check the DataFrame before saving to ensure all columns are populated\n",
    "# print(df.head())\n",
    "\n",
    "# # Save the updated DataFrame to a new CSV file\n",
    "# output_path = 'output_file_RAG_Vicuna.csv'  # This will save the file in the same folder as your notebook\n",
    "# df.to_csv(output_path, index=False)\n",
    "# print(f\"Metrics have been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f51275bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: xlrd in /home/student/venv/lib/python3.11/site-packages (2.0.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install xlrd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7f2427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/student/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error generating response for question 'What is cryptocurrency?': [Errno 111] Connection refused\n",
      "Question: What is cryptocurrency?\n",
      "Model Response: No response\n",
      "\n",
      "Q no. : 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6075c6d6c8db4c74a0279596750845d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76a6f742a4e9431391cd3024509c8be9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5aaeb89d4f4402384cebfbb35209713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a3fccbd0e954ed8bdbe71d527f67afd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3027ce54542f4fbcb59f935459d7305b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 74\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Calculate BLEU, ROUGE, and BERTScore\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m#bleu = calculate_bleu(gold_answer, model_response)\u001b[39;00m\n\u001b[1;32m     73\u001b[0m rouge \u001b[38;5;241m=\u001b[39m calculate_rouge(gold_answer, model_response)\n\u001b[0;32m---> 74\u001b[0m bertscore \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_bertscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgold_answer\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# Calculate the final weighted score\u001b[39;00m\n\u001b[1;32m     77\u001b[0m final_score \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     78\u001b[0m     weight_bleu \u001b[38;5;241m*\u001b[39m bleu \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     79\u001b[0m     weight_rouge \u001b[38;5;241m*\u001b[39m rouge \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m     80\u001b[0m     weight_bertscore \u001b[38;5;241m*\u001b[39m bertscore\n\u001b[1;32m     81\u001b[0m )\n",
      "Cell \u001b[0;32mIn[22], line 24\u001b[0m, in \u001b[0;36mcalculate_bertscore\u001b[0;34m(references, candidates)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcalculate_bertscore\u001b[39m(references, candidates):\n\u001b[0;32m---> 24\u001b[0m     P, R, F1 \u001b[38;5;241m=\u001b[39m \u001b[43mbert_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlang\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43men\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbert-base-uncased\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Smaller model\u001b[39;49;00m\n\u001b[1;32m     28\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrescale_with_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F1\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/bert_score/score.py:101\u001b[0m, in \u001b[0;36mscore\u001b[0;34m(cands, refs, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, lang, return_hash, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 101\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m idf:\n\u001b[1;32m    104\u001b[0m     idf_dict \u001b[38;5;241m=\u001b[39m defaultdict(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;241m1.0\u001b[39m)\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/transformers/modeling_utils.py:3157\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3152\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   3153\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   3154\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3155\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3156\u001b[0m         )\n\u001b[0;32m-> 3157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1174\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1171\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1172\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:780\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    779\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 780\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    783\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    784\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    785\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:805\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    803\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 805\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    808\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1160\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1155\u001b[0m             device,\n\u001b[1;32m   1156\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1157\u001b[0m             non_blocking,\n\u001b[1;32m   1158\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1159\u001b[0m         )\n\u001b[0;32m-> 1160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: uncorrectable ECC error encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Ensure NLTK models are downloaded\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Function to calculate BLEU score\n",
    "def calculate_bleu(reference, candidate):\n",
    "    reference_tokens = [nltk.word_tokenize(reference)]\n",
    "    candidate_tokens = nltk.word_tokenize(candidate)\n",
    "    return sentence_bleu(reference_tokens, candidate_tokens)\n",
    "\n",
    "# Function to calculate ROUGE-L score\n",
    "def calculate_rouge(reference, candidate):\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, candidate)\n",
    "    return scores['rougeL'].fmeasure\n",
    "\n",
    "# Function to calculate BERTScore\n",
    "def calculate_bertscore(references, candidates):\n",
    "    P, R, F1 = bert_score(\n",
    "        candidates, references, \n",
    "        lang='en', \n",
    "        model_type='bert-base-uncased',  # Smaller model\n",
    "        rescale_with_baseline=True\n",
    "    )\n",
    "    return F1.mean().item()\n",
    "\n",
    "# Weights for final score calculation\n",
    "#weight_bleu = 0.3 \n",
    "weight_rouge = 0.3\n",
    "weight_bertscore = 0.7\n",
    "\n",
    "# Load/change CSV file\n",
    "excel_path = 'Long_Ans_Q&A_45.csv'  # Path to your CSV file\n",
    "#excel_path = 'QandA_Short.csv'\n",
    "df = pd.read_csv(excel_path)\n",
    "\n",
    "# Define column names based on your CSV file structure\n",
    "question_column = 'Questions'         # Column containing questions\n",
    "gold_answer_column = 'Answers'   # Column containing gold standard answers\n",
    "\n",
    "# Lists to store evaluation metrics and model responses\n",
    "bleu_scores = []\n",
    "rouge_scores = []\n",
    "bert_scores = []\n",
    "final_scores = []\n",
    "model_responses = []\n",
    "\n",
    "# Loop through each question in the DataFrame\n",
    "for i, row in df.iterrows():\n",
    "    question = row[question_column]\n",
    "    gold_answer = row[gold_answer_column]\n",
    "\n",
    "    # Generate the model's response with error handling\n",
    "    try:\n",
    "        model_response = query_rag(question) or \"No response\"  # Use \"No response\" if the function returns None\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating response for question '{question}': {e}\")\n",
    "        model_response = \"No response\"  # Default response on failure\n",
    "\n",
    "    # Strip leading/trailing whitespace or newlines\n",
    "    model_response = model_response.strip()\n",
    "    print(f\"Question: {question}\\nModel Response: {model_response}\\n\")  # Print to check the response\n",
    "    print(\"Q no. :\",i)\n",
    "    model_responses.append(model_response)\n",
    "\n",
    "    # Calculate BLEU, ROUGE, and BERTScore\n",
    "    #bleu = calculate_bleu(gold_answer, model_response)\n",
    "    rouge = calculate_rouge(gold_answer, model_response)\n",
    "    bertscore = calculate_bertscore([gold_answer], [model_response])\n",
    "\n",
    "    # Calculate the final weighted score\n",
    "    final_score = (\n",
    "        #weight_bleu * bleu +\n",
    "        weight_rouge * rouge +\n",
    "        weight_bertscore * bertscore\n",
    "    )\n",
    "\n",
    "    # Append scores to respective lists\n",
    "    #bleu_scores.append(bleu)\n",
    "    rouge_scores.append(rouge)\n",
    "    bert_scores.append(bertscore)\n",
    "    final_scores.append(final_score)\n",
    "\n",
    "# Write model responses and scores back to DataFrame\n",
    "df['Model Response'] = model_responses\n",
    "#df['BLEU Score'] = bleu_scores\n",
    "df['ROUGE-L Score'] = rouge_scores\n",
    "df['BERTScore'] = bert_scores\n",
    "df['Final Score'] = final_scores\n",
    "\n",
    "# Check the DataFrame before saving to ensure all columns are populated\n",
    "print(df.head())\n",
    "\n",
    "# Save the updated DataFrame to a new CSV file\n",
    "output_path = 'final_output_file_RAG_vicuna_long_45.csv'  # This will save the file in the same folder as your notebook\n",
    "df.to_csv(output_path, index=False)\n",
    "print(f\"Metrics have been saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff5f237",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8c76df38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 18 00:50:02 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L40S                    On  | 00000000:63:00.0 Off |                    3 |\n",
      "| N/A   38C    P0              79W / 350W |   3214MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98e33e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\r\n",
      "Requirement already satisfied: psutil in /home/student/venv/lib/python3.11/site-packages (5.9.5)\r\n"
     ]
    }
   ],
   "source": [
    "# !pip install psutil\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fc86ff89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import psutil\n",
    "\n",
    "# def clear_gpu_memory():\n",
    "#     for proc in psutil.process_iter(attrs=['pid', 'name', 'cmdline']):\n",
    "#         try:\n",
    "#             cmdline = proc.info['cmdline']\n",
    "#             if cmdline and any(\"cuda\" in arg or \"gpu\" in arg for arg in cmdline):\n",
    "#                 print(f\"Killing process {proc.info['name']} (PID: {proc.info['pid']})\")\n",
    "#                 os.kill(proc.info['pid'], 9)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Could not kill process: {e}\")\n",
    "\n",
    "# clear_gpu_memory()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e948044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Nov 18 00:53:59 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L40S                    On  | 00000000:63:00.0 Off |                    3 |\n",
      "| N/A   37C    P0              79W / 350W |   3214MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de7b52b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
